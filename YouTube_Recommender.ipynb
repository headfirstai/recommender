{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YouTube_Recommender.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyRD+wotXEFCbwkUMVguFz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qn1Lyqvw3Gf",
        "colab_type": "text"
      },
      "source": [
        "Â© HeadFirst AI, 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpOrlu6b65zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Run to set up: load model + data\n",
        "from bs4 import BeautifulSoup\n",
        "import gensim\n",
        "from IPython.display import display, HTML\n",
        "import numpy\n",
        "from operator import itemgetter\n",
        "import pandas\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "from pprint import pprint\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')  \n",
        "warnings.filterwarnings(action='ignore',category=FutureWarning,module='gensim')  \n",
        "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
        "\n",
        "stopwords = set([\n",
        "  'i',\n",
        "  'me',\n",
        "  'my',\n",
        "  'myself',\n",
        "  'we',\n",
        "  'our',\n",
        "  'ours',\n",
        "  'ourselves',\n",
        "  'you',\n",
        "  'you re',\n",
        "  'you ve',\n",
        "  'you ll',\n",
        "  'you d',\n",
        "  'your',\n",
        "  'yours',\n",
        "  'yourself',\n",
        "  'yourselves',\n",
        "  'he',\n",
        "  'him',\n",
        "  'his',\n",
        "  'himself',\n",
        "  'she',\n",
        "  'she s',\n",
        "  'her',\n",
        "  'hers',\n",
        "  'herself',\n",
        "  'it',\n",
        "  'it s',\n",
        "  'its',\n",
        "  'itself',\n",
        "  'they',\n",
        "  'them',\n",
        "  'their',\n",
        "  'theirs',\n",
        "  'themselves',\n",
        "  'what',\n",
        "  'which',\n",
        "  'who',\n",
        "  'whom',\n",
        "  'this',\n",
        "  'that',\n",
        "  'that ll',\n",
        "  'these',\n",
        "  'those',\n",
        "  'am',\n",
        "  'is',\n",
        "  'are',\n",
        "  'was',\n",
        "  'were',\n",
        "  'be',\n",
        "  'been',\n",
        "  'being',\n",
        "  'have',\n",
        "  'has',\n",
        "  'had',\n",
        "  'having',\n",
        "  'do',\n",
        "  'does',\n",
        "  'did',\n",
        "  'doing',\n",
        "  'a',\n",
        "  'an',\n",
        "  'the',\n",
        "  'and',\n",
        "  'but',\n",
        "  'if',\n",
        "  'or',\n",
        "  'because',\n",
        "  'as',\n",
        "  'until',\n",
        "  'while',\n",
        "  'of',\n",
        "  'at',\n",
        "  'by',\n",
        "  'for',\n",
        "  'with',\n",
        "  'about',\n",
        "  'against',\n",
        "  'between',\n",
        "  'into',\n",
        "  'through',\n",
        "  'during',\n",
        "  'before',\n",
        "  'after',\n",
        "  'above',\n",
        "  'below',\n",
        "  'to',\n",
        "  'from',\n",
        "  'up',\n",
        "  'down',\n",
        "  'in',\n",
        "  'out',\n",
        "  'on',\n",
        "  'off',\n",
        "  'over',\n",
        "  'under',\n",
        "  'again',\n",
        "  'further',\n",
        "  'then',\n",
        "  'once',\n",
        "  'here',\n",
        "  'there',\n",
        "  'when',\n",
        "  'where',\n",
        "  'why',\n",
        "  'how',\n",
        "  'all',\n",
        "  'any',\n",
        "  'both',\n",
        "  'each',\n",
        "  'few',\n",
        "  'more',\n",
        "  'most',\n",
        "  'other',\n",
        "  'some',\n",
        "  'such',\n",
        "  'no',\n",
        "  'nor',\n",
        "  'not',\n",
        "  'only',\n",
        "  'own',\n",
        "  'same',\n",
        "  'so',\n",
        "  'than',\n",
        "  'too',\n",
        "  'very',\n",
        "  's',\n",
        "  't',\n",
        "  'can',\n",
        "  'will',\n",
        "  'just',\n",
        "  'don',\n",
        "  'don t',\n",
        "  'should',\n",
        "  'should ve',\n",
        "  'now',\n",
        "  'd',\n",
        "  'll',\n",
        "  'm',\n",
        "  'o',\n",
        "  're',\n",
        "  've',\n",
        "  'y',\n",
        "  'ain',\n",
        "  'aren',\n",
        "  'aren t',\n",
        "  'couldn',\n",
        "  'couldn t',\n",
        "  'didn',\n",
        "  'didn t',\n",
        "  'doesn',\n",
        "  'doesn t',\n",
        "  'hadn',\n",
        "  'hadn t',\n",
        "  'hasn',\n",
        "  'hasn t',\n",
        "  'haven',\n",
        "  'haven t',\n",
        "  'isn',\n",
        "  'isn t',\n",
        "  'ma',\n",
        "  'mightn',\n",
        "  'mightn t',\n",
        "  'mustn',\n",
        "  'mustn t',\n",
        "  'needn',\n",
        "  'needn t',\n",
        "  'shan',\n",
        "  'shan t',\n",
        "  'shouldn',\n",
        "  'shouldn t',\n",
        "  'wasn',\n",
        "  'wasn t',\n",
        "  'weren',\n",
        "  'weren t',\n",
        "  'won',\n",
        "  'won t',\n",
        "  'wouldn',\n",
        "  'wouldn t'\n",
        "])\n",
        "\n",
        "def youtube_url_from_id(youtube_id):\n",
        "  return f\"https://www.youtube.com/watch?v={youtube_id}\"\n",
        "\n",
        "def scrape_youtube_info(youtube_id):\n",
        "  row = {\"video_id\": youtube_id}\n",
        "\n",
        "  url = youtube_url_from_id(youtube_id)\n",
        "  source = requests.get(url).text\n",
        "  soup = BeautifulSoup(source, 'lxml')\n",
        "\n",
        "  title = soup.find(\"title\")\n",
        "  row[\"title\"] = title.text if title is not None else \"None\"\n",
        "\n",
        "  meta_tags = \"|\".join([tag[\"content\"] for tag in soup.findAll(\"meta\", property=\"og:video:tag\")])\n",
        "  row[\"tags\"] = meta_tags\n",
        "\n",
        "  view_count = soup.find(\"meta\", itemprop=\"interactionCount\")\n",
        "  row[\"views\"] = int(view_count[\"content\"]) if view_count is not None else 0.\n",
        "\n",
        "  description = soup.find(\"meta\", itemprop=\"description\")\n",
        "  row[\"description\"] = description[\"content\"] if description is not None else \"None\"\n",
        "\n",
        "  like_count = re.search(\"\\\"label\\\":\\\"([\\d,]+) likes\\\"\", source)\n",
        "  row[\"likes\"] = int(like_count.group(1).replace(\",\", \"\")) if like_count is not None else 0.\n",
        "\n",
        "  dislike_count = re.search(\"\\\"label\\\":\\\"([\\d,]+) dislikes\\\"\", source)\n",
        "  row[\"dislikes\"] = int(dislike_count.group(1).replace(\",\", \"\")) if dislike_count is not None else 0.\n",
        "\n",
        "  category = re.search(\"\\\"category\\\":\\\"(\\w+)\\\"\", source)\n",
        "  row[\"category\"] = category.group(1) if category is not None else 0.\n",
        "\n",
        "  channel_id = soup.find(\"meta\", itemprop=\"channelId\")\n",
        "  if channel_id is not None:\n",
        "    channel_id_str = channel_id[\"content\"]\n",
        "    channel_url = f\"https://www.youtube.com/channel/{channel_id_str}\"\n",
        "    channel_source = requests.get(channel_url).text\n",
        "    channel_soup = BeautifulSoup(channel_source, 'lxml')\n",
        "\n",
        "    channel_title = channel_soup.find(\"title\")\n",
        "    row[\"channel_title\"] = channel_title.text if channel_title is not None else \"None\"\n",
        "  else:\n",
        "    row[\"channel_title\"] = \"None\"\n",
        "\n",
        "  row = {k: [v] for k, v in row.items()}\n",
        "  return pandas.DataFrame.from_dict(row)\n",
        "\n",
        "def get_embedding(model, sentence):\n",
        "  words = preprocess_tokenize(sentence)\n",
        "\n",
        "  mean_embedding = get_mean_vector(model, words)\n",
        "\n",
        "  return mean_embedding\n",
        "\n",
        "def preprocess_tokenize(sentence):\n",
        "  # Restrict to alphanumeric\n",
        "  sentence = ''.join([ch if ch.isalnum() else ' ' for ch in str(sentence)])\n",
        "  sentence = sentence.strip()\n",
        "\n",
        "  # Tokenize and remove stopwords\n",
        "  words = []\n",
        "  tokens = sentence.split()\n",
        "\n",
        "  for token in tokens:\n",
        "    if len(token) == 0 or token in stopwords:\n",
        "      continue\n",
        "\n",
        "    token_words = [token_word.lower() for token_word in split_camel(token)]\n",
        "    words.extend(token_words)\n",
        "\n",
        "  return words\n",
        "\n",
        "def split_camel(token):\n",
        "  return re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', token)).split()\n",
        "\n",
        "def get_mean_vector(model, words):\n",
        "  # remove out-of-vocabulary words\n",
        "  words = [word for word in words if word in model.vocab]\n",
        "  if len(words) >= 1:\n",
        "    return numpy.mean(model[words], axis=0)\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def get_random_video_id(videos):\n",
        "  return videos.sample().iloc[0][\"video_id\"]\n",
        "\n",
        "def semantic_similarity(column, query_row, candidate_row):\n",
        "  if query_row[column] == candidate_row[column]:\n",
        "    return 1.\n",
        "\n",
        "  semantic_column = f'{column}_semantic'\n",
        "\n",
        "  vector1 = query_row[semantic_column]\n",
        "  vector2 = get_embedding(model, candidate_row[column])\n",
        "\n",
        "  if vector1 is None or vector2 is None:\n",
        "    return 0.\n",
        "  \n",
        "  # TODO: to downsize, we can implement these methods ourselves\n",
        "  return numpy.dot(vector1, vector2)/(numpy.linalg.norm(vector1)* numpy.linalg.norm(vector2))\n",
        "\n",
        "# Assumes that videos are deduplicated by video_id\n",
        "def find_by_id(youtube_id, videos):\n",
        "  query_df = videos.loc[videos['video_id'] == youtube_id]\n",
        "  \n",
        "  if len(query_df) == 0:\n",
        "    return None\n",
        "  \n",
        "  return query_df\n",
        "\n",
        "def suggest_similar(youtube_id, videos, ranking_methods):\n",
        "  query_df = find_by_id(youtube_id, videos)\n",
        "\n",
        "  if query_df is None:\n",
        "    query_df = scrape_youtube_info(youtube_id)\n",
        "\n",
        "  query_row = query_df.iloc[0]\n",
        "\n",
        "  # TODO: compute features on query_row\n",
        "  for column, method in ranking_methods.items():\n",
        "    if method == \"semantic\":\n",
        "      semantic_column = f'{column}_semantic'\n",
        "      query_row[semantic_column] = get_embedding(model, query_row[column])\n",
        "\n",
        "  index_to_similarity = {}\n",
        "  for tup in videos.itertuples():\n",
        "    index = tup.Index\n",
        "    candidate_row = videos.iloc[index]\n",
        "\n",
        "    if query_row[\"video_id\"] == candidate_row[\"video_id\"]:\n",
        "      continue\n",
        "    \n",
        "    similarities = []\n",
        "    for column, method in ranking_methods.items():\n",
        "      if method == \"semantic\":\n",
        "        similarities.append(\n",
        "          semantic_similarity(column, query_row, candidate_row)\n",
        "        )\n",
        "    \n",
        "    index_to_similarity[index] = numpy.mean(similarities)\n",
        "  \n",
        "  top = sorted(index_to_similarity.items(), key=itemgetter(1), reverse=True)[:5]\n",
        "  return top\n",
        "\n",
        "# Load Model\n",
        "start = time.time()\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('./data/vectors.bin.gz', binary=True)\n",
        "end = time.time()\n",
        "print(f\"Loaded model in {end - start} seconds\")\n",
        "\n",
        "# Load Data\n",
        "start = time.time()\n",
        "videos = pandas.read_csv('./data/USvideos.csv')\n",
        "videos = videos.drop_duplicates(subset=['video_id'])\n",
        "videos = videos.reset_index(drop=True)\n",
        "end = time.time()\n",
        "print(f\"Loaded YouTube data in {end - start} seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bACQris-lvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We can define our model here in the format `data: similarity measure`\n",
        "similarity_features = {\n",
        "  \"title\": \"semantic\",\n",
        "  #\"tags\": \"semantic\",\n",
        "  #\"channel_title\": \"semantic\",\n",
        "  #\"description\": \"semantic\",\n",
        "  #\"category\": \"semantic\",\n",
        "}\n",
        "\n",
        "input_youtube_id = get_random_video_id(videos)\n",
        "\n",
        "## We can also suggest similar videos for any YouTube ID\n",
        "## A YouTube ID is the last part of the video URL\n",
        "## e.g. the ID for https://www.youtube.com/watch?v=dQw4w9WgXcQ would be dQw4w9WgXcQ\n",
        "## Uncomment this line below to specify an ID\n",
        "# input_youtube_id = \"dQw4w9WgXcQ\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I24RWi9BuHh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get suggestions for most similar videos\n",
        "start = time.time()\n",
        "top_id_similarities = suggest_similar(input_youtube_id, videos, similarity_features)\n",
        "end = time.time()\n",
        "print(f\"Computed suggestions in {end - start} seconds\")\n",
        "\n",
        "if input_youtube_id in videos[\"video_id\"]:\n",
        "  input_df = find_by_id(input_youtube_id, videos).copy()\n",
        "else:\n",
        "  input_df = scrape_youtube_info(input_youtube_id)\n",
        "output_df = videos.iloc[[t[0] for t in top_id_similarities]].copy()\n",
        "\n",
        "input_df[\"url\"] = input_df[\"video_id\"].apply(lambda x: youtube_url_from_id(x))\n",
        "output_df[\"url\"] = output_df[\"video_id\"].apply(lambda x: youtube_url_from_id(x))\n",
        "output_df.insert(0, \"score\", [round(t[1], 3) for t in top_id_similarities])\n",
        "\n",
        "input_df = input_df.drop(columns=[\"video_id\"])\n",
        "output_df = output_df.drop(columns=[\"video_id\", \"tags\", \"likes\", \"dislikes\", \"comment_count\", \"description\"])\n",
        "\n",
        "display(HTML(input_df.to_html(index=False, notebook=True, render_links=True, justify=\"left\")))\n",
        "with pandas.option_context('display.max_colwidth', 75):\n",
        "  display(HTML(output_df.to_html(index=False, notebook=True, render_links=True, justify=\"left\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD2xQykdaRGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dog = model['dog']\n",
        "print(dog.shape)\n",
        "print(dog[:10])\n",
        "\n",
        "# Deal with an out of dictionary word: CaseyNeistat\n",
        "if 'Casey' in model:\n",
        "    print(model['Casey'].shape)\n",
        "else:\n",
        "    print('{0} is an out of dictionary word'.format('Casey'))\n",
        "\n",
        "\n",
        "# Some predefined functions that show content related information for given words\n",
        "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n",
        "\n",
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
        "\n",
        "print(model.similarity('CaseyNeistat', 'CaseyNeistat'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}