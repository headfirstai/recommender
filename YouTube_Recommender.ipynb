{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YouTube_Recommender.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNfsr6IoCOV73WdC9ZF7M06"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qn1Lyqvw3Gf",
        "colab_type": "text"
      },
      "source": [
        "Â© HeadFirst AI, 2020\n",
        "\n",
        "# Building a YouTube Video Recommender\n",
        "Let's build our own AI for making YouTube video recommendations! This notebook contains data for 6,000 YouTube videos that were trending from 2017 to 2018. By experimenting with various data and features, can you design the best recommender system?\n",
        "\n",
        "**To get started, run the cell below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpOrlu6b65zy",
        "colab_type": "code",
        "code_folding": [
          0
        ],
        "colab": {}
      },
      "source": [
        "## Run to set up: load model + data\n",
        "from bs4 import BeautifulSoup\n",
        "import gensim\n",
        "from IPython.display import display, HTML\n",
        "import numpy\n",
        "from operator import itemgetter\n",
        "import pandas\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "from pprint import pprint\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "print(\"Loading model...\")\n",
        "\n",
        "warnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')  \n",
        "warnings.filterwarnings(action='ignore',category=FutureWarning,module='gensim')  \n",
        "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
        "\n",
        "stopwords = set([\n",
        "  'i',\n",
        "  'me',\n",
        "  'my',\n",
        "  'myself',\n",
        "  'we',\n",
        "  'our',\n",
        "  'ours',\n",
        "  'ourselves',\n",
        "  'you',\n",
        "  'you re',\n",
        "  'you ve',\n",
        "  'you ll',\n",
        "  'you d',\n",
        "  'your',\n",
        "  'yours',\n",
        "  'yourself',\n",
        "  'yourselves',\n",
        "  'he',\n",
        "  'him',\n",
        "  'his',\n",
        "  'himself',\n",
        "  'she',\n",
        "  'she s',\n",
        "  'her',\n",
        "  'hers',\n",
        "  'herself',\n",
        "  'it',\n",
        "  'it s',\n",
        "  'its',\n",
        "  'itself',\n",
        "  'they',\n",
        "  'them',\n",
        "  'their',\n",
        "  'theirs',\n",
        "  'themselves',\n",
        "  'what',\n",
        "  'which',\n",
        "  'who',\n",
        "  'whom',\n",
        "  'this',\n",
        "  'that',\n",
        "  'that ll',\n",
        "  'these',\n",
        "  'those',\n",
        "  'am',\n",
        "  'is',\n",
        "  'are',\n",
        "  'was',\n",
        "  'were',\n",
        "  'be',\n",
        "  'been',\n",
        "  'being',\n",
        "  'have',\n",
        "  'has',\n",
        "  'had',\n",
        "  'having',\n",
        "  'do',\n",
        "  'does',\n",
        "  'did',\n",
        "  'doing',\n",
        "  'a',\n",
        "  'an',\n",
        "  'the',\n",
        "  'and',\n",
        "  'but',\n",
        "  'if',\n",
        "  'or',\n",
        "  'because',\n",
        "  'as',\n",
        "  'until',\n",
        "  'while',\n",
        "  'of',\n",
        "  'at',\n",
        "  'by',\n",
        "  'for',\n",
        "  'with',\n",
        "  'about',\n",
        "  'against',\n",
        "  'between',\n",
        "  'into',\n",
        "  'through',\n",
        "  'during',\n",
        "  'before',\n",
        "  'after',\n",
        "  'above',\n",
        "  'below',\n",
        "  'to',\n",
        "  'from',\n",
        "  'up',\n",
        "  'down',\n",
        "  'in',\n",
        "  'out',\n",
        "  'on',\n",
        "  'off',\n",
        "  'over',\n",
        "  'under',\n",
        "  'again',\n",
        "  'further',\n",
        "  'then',\n",
        "  'once',\n",
        "  'here',\n",
        "  'there',\n",
        "  'when',\n",
        "  'where',\n",
        "  'why',\n",
        "  'how',\n",
        "  'all',\n",
        "  'any',\n",
        "  'both',\n",
        "  'each',\n",
        "  'few',\n",
        "  'more',\n",
        "  'most',\n",
        "  'other',\n",
        "  'some',\n",
        "  'such',\n",
        "  'no',\n",
        "  'nor',\n",
        "  'not',\n",
        "  'only',\n",
        "  'own',\n",
        "  'same',\n",
        "  'so',\n",
        "  'than',\n",
        "  'too',\n",
        "  'very',\n",
        "  's',\n",
        "  't',\n",
        "  'can',\n",
        "  'will',\n",
        "  'just',\n",
        "  'don',\n",
        "  'don t',\n",
        "  'should',\n",
        "  'should ve',\n",
        "  'now',\n",
        "  'd',\n",
        "  'll',\n",
        "  'm',\n",
        "  'o',\n",
        "  're',\n",
        "  've',\n",
        "  'y',\n",
        "  'ain',\n",
        "  'aren',\n",
        "  'aren t',\n",
        "  'couldn',\n",
        "  'couldn t',\n",
        "  'didn',\n",
        "  'didn t',\n",
        "  'doesn',\n",
        "  'doesn t',\n",
        "  'hadn',\n",
        "  'hadn t',\n",
        "  'hasn',\n",
        "  'hasn t',\n",
        "  'haven',\n",
        "  'haven t',\n",
        "  'isn',\n",
        "  'isn t',\n",
        "  'ma',\n",
        "  'mightn',\n",
        "  'mightn t',\n",
        "  'mustn',\n",
        "  'mustn t',\n",
        "  'needn',\n",
        "  'needn t',\n",
        "  'shan',\n",
        "  'shan t',\n",
        "  'shouldn',\n",
        "  'shouldn t',\n",
        "  'wasn',\n",
        "  'wasn t',\n",
        "  'weren',\n",
        "  'weren t',\n",
        "  'won',\n",
        "  'won t',\n",
        "  'wouldn',\n",
        "  'wouldn t'\n",
        "])\n",
        "\n",
        "def youtube_url_from_id(youtube_id):\n",
        "  return f\"https://www.youtube.com/watch?v={youtube_id}\"\n",
        "\n",
        "def scrape_youtube_info(youtube_id):\n",
        "  row = {\"video_id\": youtube_id}\n",
        "\n",
        "  url = youtube_url_from_id(youtube_id)\n",
        "  source = requests.get(url).text\n",
        "  soup = BeautifulSoup(source, 'lxml')\n",
        "\n",
        "  title = soup.find(\"title\")\n",
        "  row[\"title\"] = title.text if title is not None else \"None\"\n",
        "\n",
        "  meta_tags = \"|\".join([tag[\"content\"] for tag in soup.findAll(\"meta\", property=\"og:video:tag\")])\n",
        "  row[\"tags\"] = meta_tags\n",
        "\n",
        "  view_count = soup.find(\"meta\", itemprop=\"interactionCount\")\n",
        "  row[\"views\"] = int(view_count[\"content\"]) if view_count is not None else 0.\n",
        "\n",
        "  description = soup.find(\"meta\", itemprop=\"description\")\n",
        "  row[\"description\"] = description[\"content\"] if description is not None else \"None\"\n",
        "\n",
        "  like_count = re.search(\"\\\"label\\\":\\\"([\\d,]+) likes\\\"\", source)\n",
        "  row[\"likes\"] = int(like_count.group(1).replace(\",\", \"\")) if like_count is not None else 0.\n",
        "\n",
        "  dislike_count = re.search(\"\\\"label\\\":\\\"([\\d,]+) dislikes\\\"\", source)\n",
        "  row[\"dislikes\"] = int(dislike_count.group(1).replace(\",\", \"\")) if dislike_count is not None else 0.\n",
        "\n",
        "  category = re.search(\"\\\"category\\\":\\\"(\\w+)\\\"\", source)\n",
        "  row[\"category\"] = category.group(1) if category is not None else 0.\n",
        "\n",
        "  channel_id = soup.find(\"meta\", itemprop=\"channelId\")\n",
        "  if channel_id is not None:\n",
        "    channel_id_str = channel_id[\"content\"]\n",
        "    channel_url = f\"https://www.youtube.com/channel/{channel_id_str}\"\n",
        "    channel_source = requests.get(channel_url).text\n",
        "    channel_soup = BeautifulSoup(channel_source, 'lxml')\n",
        "\n",
        "    channel_title = channel_soup.find(\"title\")\n",
        "    row[\"channel_title\"] = channel_title.text if channel_title is not None else \"None\"\n",
        "  else:\n",
        "    row[\"channel_title\"] = \"None\"\n",
        "\n",
        "  row = {k: [v] for k, v in row.items()}\n",
        "  return pandas.DataFrame.from_dict(row)\n",
        "\n",
        "def get_embedding(model, sentence):\n",
        "  words = preprocess_tokenize(sentence)\n",
        "\n",
        "  mean_embedding = get_mean_vector(model, words)\n",
        "\n",
        "  return mean_embedding\n",
        "\n",
        "def preprocess_tokenize(sentence):\n",
        "  # Restrict to alphanumeric\n",
        "  sentence = ''.join([ch if ch.isalnum() else ' ' for ch in str(sentence)])\n",
        "  sentence = sentence.strip()\n",
        "\n",
        "  # Tokenize and remove stopwords\n",
        "  words = []\n",
        "  tokens = sentence.split()\n",
        "\n",
        "  for token in tokens:\n",
        "    if len(token) == 0 or token in stopwords:\n",
        "      continue\n",
        "\n",
        "    token_words = [token_word.lower() for token_word in split_camel(token)]\n",
        "    words.extend(token_words)\n",
        "\n",
        "  return words\n",
        "\n",
        "def split_camel(token):\n",
        "  return re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', token)).split()\n",
        "\n",
        "def get_mean_vector(model, words):\n",
        "  # remove out-of-vocabulary words\n",
        "  words = [word for word in words if word in model.vocab]\n",
        "  if len(words) >= 1:\n",
        "    return numpy.mean(model[words], axis=0)\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def get_random_video_id(videos):\n",
        "  return videos.sample().iloc[0][\"video_id\"]\n",
        "\n",
        "def semantic_similarity(column, query_row, candidate_row):\n",
        "  if query_row[column] == candidate_row[column]:\n",
        "    return 1.\n",
        "\n",
        "  semantic_column = f'{column}_semantic'\n",
        "\n",
        "  vector1 = query_row[semantic_column]\n",
        "  vector2 = get_embedding(model, candidate_row[column])\n",
        "\n",
        "  if vector1 is None or vector2 is None:\n",
        "    return 0.\n",
        "  \n",
        "  # TODO: to downsize, we can implement these methods ourselves\n",
        "  return numpy.dot(vector1, vector2)/(numpy.linalg.norm(vector1)* numpy.linalg.norm(vector2))\n",
        "\n",
        "def edit_distance(s1, s2):\n",
        "    if len(s1) > len(s2):\n",
        "        s1, s2 = s2, s1\n",
        "\n",
        "    distances = range(len(s1) + 1)\n",
        "    for i2, c2 in enumerate(s2):\n",
        "        distances_ = [i2+1]\n",
        "        for i1, c1 in enumerate(s1):\n",
        "            if c1 == c2:\n",
        "                distances_.append(distances[i1])\n",
        "            else:\n",
        "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
        "        distances = distances_\n",
        "    return distances[-1]\n",
        "\n",
        "def character_similarity(column, query_row, candidate_row):\n",
        "  if query_row[column] == candidate_row[column]:\n",
        "    return 1.\n",
        "\n",
        "  s1 = str(query_row[column])[:20]\n",
        "  s2 = str(candidate_row[column])[:20]\n",
        "  denominator = 1. * (len(s1) + len(s2))\n",
        "  \n",
        "  return 1. - edit_distance(s1, s2) / denominator\n",
        "\n",
        "def numeric_similarity(column, query_row, candidate_row):\n",
        "  if query_row[column] == candidate_row[column]:\n",
        "    return 1.\n",
        "\n",
        "  try:\n",
        "    num1 = float(query_row[column])\n",
        "    num2 = float(candidate_row[column])\n",
        "\n",
        "    return 1. - abs(num1 - num2) / max([num1, num2])\n",
        "  except:\n",
        "    return 0.\n",
        "\n",
        "# Assumes that videos are deduplicated by video_id\n",
        "def find_by_id(youtube_id, videos):\n",
        "  query_df = videos.loc[videos['video_id'] == youtube_id]\n",
        "  \n",
        "  if len(query_df) == 0:\n",
        "    return None\n",
        "  \n",
        "  return query_df\n",
        "\n",
        "def suggest_similar(youtube_id, videos, ranking_methods):\n",
        "  query_df = find_by_id(youtube_id, videos)\n",
        "\n",
        "  if query_df is None:\n",
        "    query_df = scrape_youtube_info(youtube_id)\n",
        "\n",
        "  query_row = query_df.iloc[0]\n",
        "\n",
        "  for column, method in ranking_methods.items():\n",
        "    if method == \"semantic\":\n",
        "      semantic_column = f'{column}_semantic'\n",
        "      query_row[semantic_column] = get_embedding(model, query_row[column])\n",
        "\n",
        "  index_to_similarity = {}\n",
        "  for tup in videos.itertuples():\n",
        "    index = tup.Index\n",
        "    candidate_row = videos.iloc[index]\n",
        "\n",
        "    if query_row[\"video_id\"] == candidate_row[\"video_id\"]:\n",
        "      continue\n",
        "    \n",
        "    similarities = []\n",
        "    for column, method in ranking_methods.items():\n",
        "      if method == \"semantic\":\n",
        "        similarities.append(\n",
        "          semantic_similarity(column, query_row, candidate_row)\n",
        "        )\n",
        "      elif method == \"character\":\n",
        "        similarities.append(\n",
        "          character_similarity(column, query_row, candidate_row)\n",
        "        )\n",
        "      elif method == \"numeric\":\n",
        "        similarities.append(\n",
        "          numeric_similarity(column, query_row, candidate_row)\n",
        "        )\n",
        "\n",
        "    index_to_similarity[index] = numpy.mean(similarities)\n",
        "  \n",
        "  top = sorted(index_to_similarity.items(), key=itemgetter(1), reverse=True)[:5]\n",
        "  return top\n",
        "\n",
        "# Load Model\n",
        "start = time.time()\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('./data/vectors.bin.gz', binary=True)\n",
        "end = time.time()\n",
        "print(f\"Loaded model in {end - start} seconds\")\n",
        "\n",
        "# Load Data\n",
        "start = time.time()\n",
        "videos = pandas.read_csv('./data/USvideos.csv')\n",
        "videos = videos.drop_duplicates(subset=['video_id'])\n",
        "videos = videos.reset_index(drop=True)\n",
        "end = time.time()\n",
        "print(f\"Loaded YouTube data in {end - start} seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aszwzW_L6WC0",
        "colab_type": "text"
      },
      "source": [
        "## Exploring Our Data\n",
        "Let's get familiar with the data we have to work with. We can do so by randomly sampling a couple of YouTube videos from our dataset.\n",
        "\n",
        "Try collecting at least 5 samples. What do you notice about the data? Do you see any features that might be useful for making recommendations?\n",
        "\n",
        "**Run the cell below to collect a sample.** (You may have to wait for the cell above to finish loading.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn_TZxzZ7dd4",
        "colab_type": "code",
        "code_folding": [
          0
        ],
        "colab": {}
      },
      "source": [
        "## Display a random sample\n",
        "input_youtube_id = get_random_video_id(videos)\n",
        "input_df = find_by_id(input_youtube_id, videos).copy()\n",
        "input_df[\"url\"] = input_df[\"video_id\"].apply(lambda x: youtube_url_from_id(x))\n",
        "input_df = input_df.drop(columns=[\"video_id\"])\n",
        "display(HTML(input_df.to_html(index=False, notebook=True, render_links=True, justify=\"left\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi9N_ujX8by_",
        "colab_type": "text"
      },
      "source": [
        "## How AI understands text\n",
        "As you can see, a lot of the data we've gathered on each YouTube is textual - such as title, tags, and description. As we discussed in class, there are a couple of ways that an AI can learn to understand text.\n",
        "\n",
        "### Character Similarity\n",
        "The **edit distance** algorithm measures how many single-character changes (removals, insertions, or substitutions) are required to transform one word into another. Edit distance is famous for applications such as spell-check and DNA sequence analysis.\n",
        "\n",
        "Let's try with a couple of words! Run the cell below to see an example of edit distance in action. Then, try substituting your own words into the code (in variables `word1` and `word2`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlYXlh1WASJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word1 = \"their\"\n",
        "word2 = \"there\"\n",
        "print(f'The edit distance between \"{word1}\" and \"{word2}\" is {edit_distance(word1, word2)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CAn2r3lB9is",
        "colab_type": "text"
      },
      "source": [
        "### Semantic Similarity\n",
        "Semantic similarity is an AI technique that measures how close two words are in _meaning_. AI models discover the meaning of words by training on tasks such as predicting the missing word in a sentence. After lots of feedback, the AI learns to represent every word it has seen by mapping each word to a vector (like a 2D point, but with more dimensions).\n",
        "\n",
        "This technique is called a **word embedding**. Once these word vectors are known, we can compute the similarity of word meanings by using distance metrics like the Pythagorean Theorem.\n",
        "\n",
        "Let's try it out! As before, run the cell below to see an example of semantic similarity in action. Then, try substituting your own words into the code (in variables `word1` and `word2`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB_dFSR8DpW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word1 = \"their\"\n",
        "word2 = \"there\"\n",
        "\n",
        "## Check to see if the model recognizes the words\n",
        "if word1 not in model:\n",
        "  print(f'The model does not recognize {word1}')\n",
        "elif word2 not in model:\n",
        "  print(f'The model does not recognize {word2}')\n",
        "else:\n",
        "  sim = model.similarity(word1, word2)\n",
        "  percentage = round(sim * 100, 1)\n",
        "  print(f'The semantic similarity between \"{word1}\" and \"{word2}\" is {percentage}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbKqu_b4GlwJ",
        "colab_type": "text"
      },
      "source": [
        "### Discussion Questions\n",
        "1. What did you notice when looking at both methods? Did anything surprise you?\n",
        "2. What are the strengths of edit distance? What are the strengths of semantic similarity?\n",
        "3. Which scenarios are best suited for each method?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqOge13hHpJt",
        "colab_type": "text"
      },
      "source": [
        "## Building a YouTube Recommender\n",
        "Now, we are ready to build an AI that can recommend YouTube videos based on textual features! In the cell below, we've provided a playground where we can customize which features our AI should use when suggesting similar videos. (Don't worry, you don't need to know how to code in order to customize the model).\n",
        "\n",
        "### Instructions: Defining the Model\n",
        "Edit the AI model by updating the lines under `similarity_features` below. Each row is in the format **\"data\": \"similarity measure\"** where \"data\" refers to an aspect of a YouTube video (such as \"title\") and \"similarity measure\" refers to the type of similarity algorithm to use (such as \"semantic\"). The model will generate suggestions according to the data and similarity measures you define.\n",
        "\n",
        "Valid options for similarity measure are included in the comments on each line (aka. the green text following the '#'). For textual data, either semantic or character similarity can be applied. For numeric data, numeric similarity can be applied (aka. percentage difference).\n",
        "\n",
        "Choosing \"none\" will omit the data from the model.\n",
        "\n",
        "### Instructions: Choosing an Input Video\n",
        "The model will read in one input video that it will try to find similar videos to (you can think of this as a video you've liked that YouTube will suggest additional videos for). By default, the model will choose a random input video from the dataset. You may also uncomment the last line to feed the model a custom YouTube ID from any video on YouTube.\n",
        "\n",
        "**Run the cell below to set your model and input video. Then, run the next cell to see your suggestions.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bACQris-lvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We can define our model here in the format `data: similarity measure`\n",
        "similarity_features = {\n",
        "  \"title\": \"character\",       # options: \"none\", \"semantic\", \"character\"\n",
        "  \"tags\": \"none\",             # options: \"none\", \"semantic\", \"character\"\n",
        "  \"channel_title\": \"none\",    # options: \"none\", \"semantic\", \"character\"\n",
        "  \"description\": \"none\",      # options: \"none\", \"semantic\", \"character\"\n",
        "  \"category\": \"none\",         # options: \"none\", \"semantic\", \"character\"\n",
        "  \"views\": \"none\",            # options: \"none\", \"numeric\"\n",
        "  \"likes\": \"none\",            # options: \"none\", \"numeric\"\n",
        "  \"dislikes\": \"none\",         # options: \"none\", \"numeric\"\n",
        "}\n",
        "\n",
        "print(\"Model defined. Ready to compute suggestions.\")\n",
        "\n",
        "input_youtube_id = get_random_video_id(videos)\n",
        "\n",
        "## We can also suggest similar videos for any YouTube ID\n",
        "## A YouTube ID is the last part of the video URL\n",
        "## e.g. the ID for https://www.youtube.com/watch?v=dQw4w9WgXcQ would be dQw4w9WgXcQ\n",
        "## Uncomment the line below to specify an ID\n",
        "# input_youtube_id = \"dQw4w9WgXcQ\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjxkxkFuMRxY",
        "colab_type": "text"
      },
      "source": [
        "**Run the cell below to see what the AI suggested!** (This may take up to a minute)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I24RWi9BuHh7",
        "colab_type": "code",
        "code_folding": [
          0
        ],
        "colab": {}
      },
      "source": [
        "## Get suggestions for most similar videos\n",
        "print(\"Computing suggestions...\")\n",
        "start = time.time()\n",
        "top_id_similarities = suggest_similar(input_youtube_id, videos, similarity_features)\n",
        "end = time.time()\n",
        "print(f\"Computed suggestions in {end - start} seconds\")\n",
        "\n",
        "if find_by_id(input_youtube_id, videos) is not None:\n",
        "  input_df = find_by_id(input_youtube_id, videos).copy()\n",
        "else:\n",
        "  input_df = scrape_youtube_info(input_youtube_id)\n",
        "output_df = videos.iloc[[t[0] for t in top_id_similarities]].copy()\n",
        "\n",
        "input_df[\"url\"] = input_df[\"video_id\"].apply(lambda x: youtube_url_from_id(x))\n",
        "output_df[\"url\"] = output_df[\"video_id\"].apply(lambda x: youtube_url_from_id(x))\n",
        "output_df.insert(0, \"score\", [round(t[1], 3) for t in top_id_similarities])\n",
        "\n",
        "input_df = input_df.drop(columns=[\"video_id\"])\n",
        "output_df = output_df.drop(columns=[\"video_id\", \"tags\", \"likes\", \"dislikes\", \"description\"])\n",
        "\n",
        "display(HTML('<h2>Input Video</h2>'))\n",
        "display(HTML(input_df.to_html(index=False, notebook=True, render_links=True, justify=\"left\")))\n",
        "with pandas.option_context('display.max_colwidth', 75):\n",
        "  display(HTML('<h2>Output Suggestions</h2>'))\n",
        "  display(HTML(output_df.to_html(index=False, notebook=True, render_links=True, justify=\"left\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBMO6wvkWiqg",
        "colab_type": "text"
      },
      "source": [
        "## Discussion Questions\n",
        "1. How did the recommendations change as the input features (data + similarity measures) were changed?\n",
        "2. In your opinion, what single feature works the best for recommending videos? Why?\n",
        "3. In your opinion, what group of features works the best for recommending videos?\n",
        "4. Suppose you are the CEO of YouTube. How would you decide how to evaluate the quality of recommendations?\n",
        "5. If you were to extend this recommender, what additional videos data would you gather? What other similarity measures can you come up with?"
      ]
    }
  ]
}
