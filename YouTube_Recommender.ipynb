{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YouTube_Recommender.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWTKc5GBjKw54Jxcv9gINo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qn1Lyqvw3Gf",
        "colab_type": "text"
      },
      "source": [
        "Â© HeadFirst AI, 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpOrlu6b65zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Run to set up: load model + data\n",
        "import gensim\n",
        "from IPython.display import display, HTML\n",
        "import numpy\n",
        "from operator import itemgetter\n",
        "import pandas\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "from pprint import pprint\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')  \n",
        "warnings.filterwarnings(action='ignore',category=FutureWarning,module='gensim')  \n",
        "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
        "\n",
        "stopwords = set([\n",
        "  'i',\n",
        "  'me',\n",
        "  'my',\n",
        "  'myself',\n",
        "  'we',\n",
        "  'our',\n",
        "  'ours',\n",
        "  'ourselves',\n",
        "  'you',\n",
        "  'you re',\n",
        "  'you ve',\n",
        "  'you ll',\n",
        "  'you d',\n",
        "  'your',\n",
        "  'yours',\n",
        "  'yourself',\n",
        "  'yourselves',\n",
        "  'he',\n",
        "  'him',\n",
        "  'his',\n",
        "  'himself',\n",
        "  'she',\n",
        "  'she s',\n",
        "  'her',\n",
        "  'hers',\n",
        "  'herself',\n",
        "  'it',\n",
        "  'it s',\n",
        "  'its',\n",
        "  'itself',\n",
        "  'they',\n",
        "  'them',\n",
        "  'their',\n",
        "  'theirs',\n",
        "  'themselves',\n",
        "  'what',\n",
        "  'which',\n",
        "  'who',\n",
        "  'whom',\n",
        "  'this',\n",
        "  'that',\n",
        "  'that ll',\n",
        "  'these',\n",
        "  'those',\n",
        "  'am',\n",
        "  'is',\n",
        "  'are',\n",
        "  'was',\n",
        "  'were',\n",
        "  'be',\n",
        "  'been',\n",
        "  'being',\n",
        "  'have',\n",
        "  'has',\n",
        "  'had',\n",
        "  'having',\n",
        "  'do',\n",
        "  'does',\n",
        "  'did',\n",
        "  'doing',\n",
        "  'a',\n",
        "  'an',\n",
        "  'the',\n",
        "  'and',\n",
        "  'but',\n",
        "  'if',\n",
        "  'or',\n",
        "  'because',\n",
        "  'as',\n",
        "  'until',\n",
        "  'while',\n",
        "  'of',\n",
        "  'at',\n",
        "  'by',\n",
        "  'for',\n",
        "  'with',\n",
        "  'about',\n",
        "  'against',\n",
        "  'between',\n",
        "  'into',\n",
        "  'through',\n",
        "  'during',\n",
        "  'before',\n",
        "  'after',\n",
        "  'above',\n",
        "  'below',\n",
        "  'to',\n",
        "  'from',\n",
        "  'up',\n",
        "  'down',\n",
        "  'in',\n",
        "  'out',\n",
        "  'on',\n",
        "  'off',\n",
        "  'over',\n",
        "  'under',\n",
        "  'again',\n",
        "  'further',\n",
        "  'then',\n",
        "  'once',\n",
        "  'here',\n",
        "  'there',\n",
        "  'when',\n",
        "  'where',\n",
        "  'why',\n",
        "  'how',\n",
        "  'all',\n",
        "  'any',\n",
        "  'both',\n",
        "  'each',\n",
        "  'few',\n",
        "  'more',\n",
        "  'most',\n",
        "  'other',\n",
        "  'some',\n",
        "  'such',\n",
        "  'no',\n",
        "  'nor',\n",
        "  'not',\n",
        "  'only',\n",
        "  'own',\n",
        "  'same',\n",
        "  'so',\n",
        "  'than',\n",
        "  'too',\n",
        "  'very',\n",
        "  's',\n",
        "  't',\n",
        "  'can',\n",
        "  'will',\n",
        "  'just',\n",
        "  'don',\n",
        "  'don t',\n",
        "  'should',\n",
        "  'should ve',\n",
        "  'now',\n",
        "  'd',\n",
        "  'll',\n",
        "  'm',\n",
        "  'o',\n",
        "  're',\n",
        "  've',\n",
        "  'y',\n",
        "  'ain',\n",
        "  'aren',\n",
        "  'aren t',\n",
        "  'couldn',\n",
        "  'couldn t',\n",
        "  'didn',\n",
        "  'didn t',\n",
        "  'doesn',\n",
        "  'doesn t',\n",
        "  'hadn',\n",
        "  'hadn t',\n",
        "  'hasn',\n",
        "  'hasn t',\n",
        "  'haven',\n",
        "  'haven t',\n",
        "  'isn',\n",
        "  'isn t',\n",
        "  'ma',\n",
        "  'mightn',\n",
        "  'mightn t',\n",
        "  'mustn',\n",
        "  'mustn t',\n",
        "  'needn',\n",
        "  'needn t',\n",
        "  'shan',\n",
        "  'shan t',\n",
        "  'shouldn',\n",
        "  'shouldn t',\n",
        "  'wasn',\n",
        "  'wasn t',\n",
        "  'weren',\n",
        "  'weren t',\n",
        "  'won',\n",
        "  'won t',\n",
        "  'wouldn',\n",
        "  'wouldn t'\n",
        "])\n",
        "\n",
        "def youtube_url_from_id(youtube_id):\n",
        "  return f\"https://www.youtube.com/watch?v={youtube_id}\"\n",
        "\n",
        "def get_embedding(model, sentence):\n",
        "  words = preprocess_tokenize(sentence)\n",
        "\n",
        "  mean_embedding = get_mean_vector(model, words)\n",
        "\n",
        "  return mean_embedding\n",
        "\n",
        "def preprocess_tokenize(sentence):\n",
        "  # Restrict to alphanumeric\n",
        "  sentence = ''.join([ch if ch.isalnum() else ' ' for ch in str(sentence)])\n",
        "  sentence = sentence.strip()\n",
        "\n",
        "  # Tokenize and remove stopwords\n",
        "  words = []\n",
        "  tokens = sentence.split()\n",
        "\n",
        "  for token in tokens:\n",
        "    if len(token) == 0 or token in stopwords:\n",
        "      continue\n",
        "\n",
        "    token_words = [token_word.lower() for token_word in split_camel(token)]\n",
        "    words.extend(token_words)\n",
        "\n",
        "  return words\n",
        "\n",
        "def split_camel(token):\n",
        "  return re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', token)).split()\n",
        "\n",
        "def get_mean_vector(model, words):\n",
        "  # remove out-of-vocabulary words\n",
        "  words = [word for word in words if word in model.vocab]\n",
        "  if len(words) >= 1:\n",
        "    return numpy.mean(model[words], axis=0)\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def get_random_video_id(videos):\n",
        "  return videos.sample().iloc[0][\"video_id\"]\n",
        "\n",
        "def semantic_similarity(column, query_row, candidate_row):\n",
        "  if query_row[column] == candidate_row[column]:\n",
        "    return 1.\n",
        "\n",
        "  semantic_column = f'{column}_semantic'\n",
        "\n",
        "  vector1 = query_row[semantic_column]\n",
        "  vector2 = get_embedding(model, candidate_row[column])\n",
        "\n",
        "  if vector1 is None or vector2 is None:\n",
        "    return 0.\n",
        "  \n",
        "  # TODO: to downsize, we can implement these methods ourselves\n",
        "  return numpy.dot(vector1, vector2)/(numpy.linalg.norm(vector1)* numpy.linalg.norm(vector2))\n",
        "\n",
        "# Assumes that videos are deduplicated by video_id\n",
        "def find_by_id(youtube_id, videos):\n",
        "  query_df = videos.loc[videos['video_id'] == youtube_id]\n",
        "  \n",
        "  if len(query_df) == 0:\n",
        "    return None\n",
        "  \n",
        "  return query_df\n",
        "\n",
        "def suggest_similar(youtube_id, videos, ranking_methods):\n",
        "  query_df = find_by_id(youtube_id, videos)\n",
        "\n",
        "  # TODO: extend to allow for external YouTube videos\n",
        "  if query_df is None:\n",
        "    return []\n",
        "\n",
        "  query_row = query_df.iloc[0]\n",
        "\n",
        "  # TODO: compute features on query_row\n",
        "  for column, method in ranking_methods.items():\n",
        "    if method == \"semantic\":\n",
        "      semantic_column = f'{column}_semantic'\n",
        "      query_row[semantic_column] = get_embedding(model, query_row[column])\n",
        "\n",
        "  index_to_similarity = {}\n",
        "  for tup in videos.itertuples():\n",
        "    index = tup.Index\n",
        "    candidate_row = videos.iloc[index]\n",
        "\n",
        "    if query_row[\"video_id\"] == candidate_row[\"video_id\"]:\n",
        "      continue\n",
        "    \n",
        "    similarities = []\n",
        "    for column, method in ranking_methods.items():\n",
        "      if method == \"semantic\":\n",
        "        similarities.append(\n",
        "          semantic_similarity(column, query_row, candidate_row)\n",
        "        )\n",
        "    \n",
        "    index_to_similarity[index] = numpy.mean(similarities)\n",
        "  \n",
        "  top = sorted(index_to_similarity.items(), key=itemgetter(1), reverse=True)[:5]\n",
        "  return top\n",
        "\n",
        "# Load Model\n",
        "start = time.time()\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('./data/vectors.bin.gz', binary=True)\n",
        "end = time.time()\n",
        "print(f\"Loaded model in {end - start} seconds\")\n",
        "\n",
        "# Load Data\n",
        "start = time.time()\n",
        "videos = pandas.read_csv('./data/USvideos.csv')\n",
        "videos = videos.drop_duplicates(subset=['video_id'])\n",
        "videos = videos.reset_index(drop=True)\n",
        "end = time.time()\n",
        "print(f\"Loaded YouTube data in {end - start} seconds\")\n",
        "\n",
        "# Compute features\n",
        "disable = '''\n",
        "start = time.time()\n",
        "\n",
        "string_columns = ['title', 'tags', 'category', 'channel_title', 'description']\n",
        "for column in string_columns:\n",
        "  semantic_column = f'{column}_semantic'\n",
        "\n",
        "  videos[semantic_column] = videos[column].apply(lambda x: get_embedding(model, x))\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Computed features in {end - start} seconds\")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bACQris-lvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Query and suggest\n",
        "ranking_methods = {\n",
        "  \"title\": \"semantic\",\n",
        "  #\"tags\": \"semantic\",\n",
        "  #\"channel_title\": \"semantic\",\n",
        "  #\"description\": \"semantic\",\n",
        "  #\"category\": \"semantic\",\n",
        "}\n",
        "\n",
        "input_youtube_id = get_random_video_id(videos)\n",
        "\n",
        "## Do not modify: code for computing suggestions\n",
        "start = time.time()\n",
        "top_id_similarities = suggest_similar(input_youtube_id, videos, ranking_methods)\n",
        "end = time.time()\n",
        "print(f\"Computed suggestions in {end - start} seconds\")\n",
        "\n",
        "input_df = find_by_id(input_youtube_id, videos).copy()\n",
        "output_df = videos.iloc[[t[0] for t in top_id_similarities]].copy()\n",
        "\n",
        "input_df[\"url\"] = input_df[\"video_id\"].apply(lambda x: youtube_url_from_id(x))\n",
        "output_df[\"url\"] = output_df[\"video_id\"].apply(lambda x: youtube_url_from_id(x))\n",
        "output_df.insert(0, \"score\", [round(t[1], 3) for t in top_id_similarities])\n",
        "\n",
        "input_df = input_df.drop(columns=[\"video_id\"])\n",
        "output_df = output_df.drop(columns=[\"video_id\", \"tags\", \"likes\", \"dislikes\", \"comment_count\", \"description\"])\n",
        "\n",
        "display(HTML(input_df.to_html(index=False, notebook=True, render_links=True, justify=\"left\")))\n",
        "with pandas.option_context('display.max_colwidth', 75):\n",
        "  display(HTML(output_df.to_html(index=False, notebook=True, render_links=True, justify=\"left\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD2xQykdaRGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dog = model['dog']\n",
        "print(dog.shape)\n",
        "print(dog[:10])\n",
        "\n",
        "# Deal with an out of dictionary word: CaseyNeistat\n",
        "if 'Casey' in model:\n",
        "    print(model['Casey'].shape)\n",
        "else:\n",
        "    print('{0} is an out of dictionary word'.format('Casey'))\n",
        "\n",
        "\n",
        "# Some predefined functions that show content related information for given words\n",
        "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n",
        "\n",
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
        "\n",
        "print(model.similarity('CaseyNeistat', 'CaseyNeistat'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}